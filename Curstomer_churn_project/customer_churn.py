# -*- coding: utf-8 -*-
"""customer_churn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16jdDkVaDK-VsmJXe3rmStSLk9a7cdly8

# importing dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
import pickle

"""1.Label encoders in sklearn are used to convert categorical labels into numerical values. This is helpful for machine learning models that require numerical input.

sklearn.preprocessing.LabelEncoder
LabelEncoder assigns a unique integer to each unique category in a column.

2.from imblearn.over_sampling import SMOTE-> to build uniformly distributed target class. as there are more no. of no in churn dataset than yes.

3.train_test_split
This function splits your dataset into training and testing sets. It is commonly used to evaluate the performance of a machine learning model by training it on one portion of the data and testing it on another.

4.cross_val_score
This function performs cross-validation, which helps estimate a model's performance by training and testing it multiple times on different subsets of the dataset.

# 2.Data loading and Understanding
"""

# load tech data into csv file
df=pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn.csv')

df.shape

df.head()

pd.set_option("display.max_columns",None) # to prevent truncating the columns

df.info()

# dropping customer id column as it is not required for modeling
df=df.drop(columns=['customerID'])

df.head(2)

df['gender'].unique()

for col in df.columns:
  print(col,df[col].unique())
  print("-"*50)

# excluding numerical columns and keeping onley categorical columns
numerical_cols_list=['tenure','MonthlyCharges','TotalCharges']

for col in df.columns:
  if(col not in numerical_cols_list):
    print(col,df[col].unique())
    print("-"*50)

#to check null valued columns

df.isnull().sum()

df["TotalCharges"]=df["TotalCharges"].astype(float)

"""error occured because there are some missing values but not represented as NAN"""

df[df["TotalCharges"]==" "]

len(df[df["TotalCharges"]==" "])

df["TotalCharges"]=df["TotalCharges"].replace({" ":"0.0"})

df["TotalCharges"]=df["TotalCharges"].astype(float)

df.head(10)

df.info()

print(df['Churn'].value_counts())

"""**Insights**:

1.customer Id remooved as not required for data modeling.

2.No missing values in dataset.

3.Missing values in Totalcharges were replaced with 0.0.

4.class imbalance identified in target.

3. Explolratory Data ananlysis:
"""

df.shape

df.columns

df.head(2)

df.describe()

"""Numerical Features analysis:

Understanding the distribution of numerical features
"""

def plot_histogram(df,column_name):
  plt.figure(figsize=(5,3))
  sns.histplot(df[column_name],kde=True)
  plt.title(f"Distribution of {column_name}")

  #calculate mean and meadian values for column
  column_mean=df[column_name].mean()
  column_median=df[column_name].median()

  #add vertical lines for mean and median values
  plt.axvline(column_mean,color='red',linestyle="--",label="Mean")
  plt.axvline(column_median,color='blue',linestyle="-",label="Meadian")

  plt.legend()
  plt.show()

plot_histogram(df,"tenure")

plot_histogram(df,"MonthlyCharges")

plot_histogram(df,"TotalCharges")

"""Checking for outliers:

In Machine Learning, outliers are data points that significantly differ from other observations in a dataset. They can arise due to variability in data, measurement errors, or rare events. Outliers can impact model performance, leading to inaccurate predictions and biases.

Types of Outliers
Univariate Outliers ‚Äì Outliers detected in a single feature (e.g., a salary of $1,000,000 when most salaries are around $50,000).

Multivariate Outliers ‚Äì Outliers detected in multiple features together (e.g., a person who is 10 years old with a salary of $100,000).

Causes of Outliers
1.Data entry errors

2.Measurement errors

3.Natural variation in data

4.Fraudulent activities (e.g., anomalies in financial transactions)

**Methods to Detect Outliers**

**1.Statistical Methods**

Z-score: Identifies data points that are far from the mean (e.g., beyond ¬±3 standard deviations).

IQR (Interquartile Range): Considers data points beyond the 1.5 √ó IQR range as outliers.

**2.Visualization Techniques**

Box plots

Scatter plots

Histogram distribution

**3.Machine Learning Techniques**

Isolation Forest (IF)

Local Outlier Factor (LOF)

One-Class SVM

**Handling Outliers**

Remove them if they are errors or not relevant.

Transform them using techniques like log transformation or normalization.

Cap them to a maximum or minimum threshold.

Use robust models that are less sensitive to outliers (e.g., Decision Trees).
"""

def plot_boxplot(df,column_name):
  plt.figure(figsize=(5,3))
  sns.boxplot(df[column_name])
  plt.title(f"Distribution of {column_name}")
  plt.ylabel(column_name)
  plt.show()

plot_boxplot(df,'tenure')

plot_boxplot(df,'MonthlyCharges')

plot_boxplot(df,'TotalCharges')

"""No outliers present here, if they were present they should be above that line.

Corelation heatmap for numerical columns:

A correlation heatmap is a great way to visualize the relationships between numerical columns in a dataset. It shows how strongly different features are related to each other, which is useful for feature selection and understanding data patterns.
"""

# corelation matrix heatmap
plt.figure(figsize=(10,8))
sns.heatmap(df[['tenure','MonthlyCharges','TotalCharges']].corr(),annot=True,cmap='coolwarm',fmt='.2f')
plt.show()

"""categorical columns or feature analysis:

Count plots for categorical columns:
"""

object_cols=df.select_dtypes(include='object').columns.to_list()
object_cols=["SeniorCitizen"]+object_cols
object_cols

for col in object_cols:
  plt.figure(figsize=(5,3))
  sns.countplot(x=df[col])
  plt.title(f"count plot of {col}")
  plt.show()

"""Which Affects ML Models More: Feature Imbalance vs Target Imbalance?
‚úÖ Target Imbalance (Class Imbalance) Affects ML Models More!


Why?

1Ô∏è‚É£ Direct Impact on Model Predictions

If the target (label) is imbalanced, the model may become biased toward the majority class and fail to learn patterns in the minority class.

Example: A fraud detection model trained on 98% non-fraud transactions will predict "No Fraud" almost always, making it useless.

2Ô∏è‚É£ Accuracy Becomes Misleading

A model predicting only the majority class can have high accuracy but low usefulness.

Example: If 95% of emails are non-spam and 5% are spam, a model predicting "Non-Spam" 100% of the time will still be 95% accurate but fail at detecting spam!

3Ô∏è‚É£ Severe Consequences in Real-World Applications

Medical Diagnosis: Missing rare disease cases can cost lives.

Anomaly Detection: Ignoring rare cybersecurity threats can lead to major security breaches.

Loan Default Prediction: Failing to identify risky borrowers can cause financial loss.

Feature Imbalance is Less Harmful
‚úî Feature imbalance affects learning efficiency, but models can still work fine.
‚úî Some algorithms like Decision Trees, Random Forest, and Neural Networks can handle imbalanced features.
‚úî We can normalize, bin, or engineer features to fix imbalance.

**4.Data Preprocessing:**

Label Encoding is a technique used to convert categorical variables into numerical form so that ML models can process them

Lable Encoding of target columns:
"""

df["Churn"]=df["Churn"].replace({"Yes":1,"No":0})

df.head(3)

print(df["Churn"].value_counts())

object_cols=df.select_dtypes(include='object').columns
object_cols

# initialize a empty dictionary to save the encoders
encoders={}

#apply label encoding and store the encoders

for col in object_cols:
  lable_encoder=LabelEncoder()
  df[col]=lable_encoder.fit_transform(df[col])
  encoders[col]=lable_encoder

# save the details in pickle file
with open("encoders.pkl","wb") as file:
  pickle.dump(encoders,file)

"""pickle.dump(encoders, file): Saves the encoders dictionary to a file (encoders.pkl).

This ensures that later, we can load the file and reuse the same encoders.

üîπ Why Use Pickle?

‚úî Preserves the Encoding for Future Use

‚úî Ensures Consistency on New Data

‚úî Avoids Recomputing Encodings Every Time
"""

encoders

df.head(10)

"""# Training and test data split"""

# spliting the features and targets
x=df.drop(columns=['Churn'])
y=df['Churn']

print(x)

#split training and test data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

print(y_train.value_counts())

"""# Synthetic monitering oversampling technique(SMOTE)"""

smote=SMOTE(random_state=42)

x_train_smote,y_train_smote=smote.fit_resample(x_train,y_train)

print(x_train_smote.shape)
print(y_train_smote.shape)

print(x_train_smote.value_counts())

print(y_train_smote.value_counts())

"""**5.Model Training**

Training with default hyper parameters
"""

# dictionary of models
models={
    "Decision Tree":DecisionTreeClassifier(random_state=42),
    "Random Forest":RandomForestClassifier(random_state=42),
    "XGBoost":XGBClassifier(random_state=42),
}

# dictionary to store cross validation results
cv_scroes={}

# perform 5-fold cross validations for each model
for model_name,model in models.items():
  print(model_name)
  print(model)
  print(" "*50)

"""**What is 5-Fold Cross-Validation?**

5-Fold Cross-Validation is a technique used to evaluate the performance of machine learning models. It helps ensure that the model's performance is not dependent on a specific subset of data and provides a more reliable estimate of how well the model generalizes to unseen data.

**How Does It Work?**

Split the Data: The dataset is randomly divided into 5 equal-sized subsets (or "folds").

Train & Validate in Iterations:

1. In each iteration, 4 folds are used for training.

2. The remaining 1 fold is used for validation (testing).

3. This process is repeated 5 times, with each fold serving as the validation set exactly once.

**Calculate Average Performance:**

 The final model performance is taken as the average of the 5 validation scores.

**Why Use 5-Fold Cross-Validation?**


It reduces the risk of overfitting compared to a simple train-test split.

It gives a more stable and reliable performance estimate.

It ensures that every data point is used for both training and testing, maximizing data efficiency.
"""

# dictionary to store cross validation results
cv_scroes={}

# perform 5-fold cross validations for each model
for model_name,model in models.items():
 print(f"Training {model_name} with default hyper parameters")
 score=cross_val_score(model,x_train_smote,y_train_smote,cv=5)
 cv_scroes[model_name]=score
 print(f"cross validation accuracy for {model_name} is {np.mean(score):.2f}")
 print("-"*50)

"""random forest gives highest accuracy compared to other models with default parameters"""

rfc=RandomForestClassifier(random_state=42)

rfc.fit(x_train_smote,y_train_smote)

"""**6. Model Evaluation**"""

# evaluate from test data
y_pred=rfc.predict(x_test)

print("Accuracy: ",accuracy_score(y_test,y_pred))
print("Confusion Matrix: \n",confusion_matrix(y_test,y_pred))
print("Classification Report: \n",classification_report(y_test,y_pred))

"""What is a Pickle File?

A pickle file is used to serialize (save) and deserialize (load) Python objects. It allows you to save a model or any Python object to a file and then load it later without having to retrain the model or recreate the object. This is useful for model persistence in machine learning, so you don't need to retrain the model every time.
"""

# save the churn model as pickle file

model_data={"model":rfc,"features_names":x.columns.tolist()}

with open("churn_model.pkl","wb") as f:
  pickle.dump(model_data,f)

"""**7.Load the saved model and Build a predictive System:**

load the saved model and the feature name
"""

with open("churn_model.pkl","rb") as f:
  model_data=pickle.load(f)

loaded_model=model_data["model"]
feature_names=model_data["features_names"]

print(loaded_model)

print(feature_names)

input_data = {
    'gender': 'Female',
    'SeniorCitizen': 0,
    'Partner': 'Yes',
    'Dependents': 'No',
    'tenure': 1,
    'PhoneService': 'No',
    'MultipleLines': 'No phone service',
    'InternetService': 'DSL',
    'OnlineSecurity': 'No',
    'OnlineBackup': 'Yes',
    'DeviceProtection': 'No',
    'TechSupport': 'No',
    'StreamingTV': 'No',
    'StreamingMovies': 'No',
    'Contract': 'Month-to-month',
    'PaperlessBilling': 'Yes',
    'PaymentMethod': 'Electronic check',
    'MonthlyCharges': 29.85,
    'TotalCharges': 29.85
}


input_data_df = pd.DataFrame([input_data])

with open("encoders.pkl", "rb") as f:
  encoders = pickle.load(f)

print(input_data_df.head())


# encode categorical featires using teh saved encoders
for column, encoder in encoders.items():
  input_data_df[column] = encoder.transform(input_data_df[column])

print("--"*50)

print(input_data_df.head())

# make a prediction
prediction = loaded_model.predict(input_data_df)
pred_prob = loaded_model.predict_proba(input_data_df)

print(prediction)

# results
print(f"Prediction: {'Churn' if prediction[0] == 1 else 'No Churn'}")
print(f"Prediciton Probability: {pred_prob}")

"""**Things to do:**

1.hyperparameter tuning

2.try different models

3.try downsampling

4.try to address the overfitting
"""

